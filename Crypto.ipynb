{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sanchitvohra/crypto-bot/blob/main/Crypto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pXLrq_Yy53oF"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import preprocessing\n",
    "import environments\n",
    "import models\n",
    "import agents\n",
    "import utils\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sj7WyYBp6OWu",
    "outputId": "ab8b4b6f-8fc4-479c-8243-ddcd27cbb3fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eognzY9c6R-C",
    "outputId": "4e2fdac6-0c36-4ee9-f449-2219aae3ca4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'crypto-bot'...\n",
      "remote: Enumerating objects: 140, done.\u001b[K\n",
      "remote: Counting objects: 100% (140/140), done.\u001b[K\n",
      "remote: Compressing objects: 100% (98/98), done.\u001b[K\n",
      "remote: Total 140 (delta 53), reused 115 (delta 30), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (140/140), 28.56 KiB | 835.00 KiB/s, done.\n",
      "Resolving deltas: 100% (53/53), done.\n",
      "fatal: not a git repository (or any of the parent directories): .git\n",
      "/content/crypto-bot\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/sanchitvohra/crypto-bot.git\n",
    "!git pull\n",
    "!mkdir /content/crypto-bot/data\n",
    "!cp /content/drive/MyDrive/crypto-bot/crypto_data.npy /content/crypto-bot/data/\n",
    "!mkdir /content/crypto-bot/checkpoints\n",
    "%cd /content/crypto-bot/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dneg05tC8kR_",
    "outputId": "8d79ccb8-5214-444f-ff98-f26a63bd0b05"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Training loop starting...\n",
      "[INFO] Training steps: 32\n",
      "[INFO] Model Optimization epochs: 100\n",
      "[INFO] Episode length: 10000\n",
      "[INFO] Action std init: 0.1\n",
      "[INFO] Action std decay: 0.005\n",
      "[INFO] Min action std: 0.005\n",
      "[INFO] Action std decay freq: 1\n",
      "[INFO] Starting balance: 1000000.0\n",
      "[INFO] Maximum trade action: 100000.0\n",
      "[INFO] Trading fee: 0.01\n",
      "[INFO] State History: 4\n",
      "[INFO] Reward Scaling: 0.0001\n",
      "[INFO] State dimension: 231\n",
      "[INFO] Action dimension: 5\n",
      "[INFO] Pytoch device: cuda:0\n",
      "[INFO] Pretrained: False\n",
      "[INFO] Model save path: checkpoints/\n",
      "[INFO] Model save frequecy: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to : Tesla K80\n"
     ]
    }
   ],
   "source": [
    "FORMAT = logging.Formatter('[%(levelname)s] %(message)s')\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "if not logDir:\n",
    "    logDir = \"logs/\"\n",
    "    logDir += \"config\" + str(len(os.listdir(logDir))).zfill(3)\n",
    "    os.mkdir(logDir)\n",
    "\n",
    "fileHandler = logging.FileHandler(\"{0}/{1}.log\".format(logDir, \"training\"))\n",
    "fileHandler.setFormatter(FORMAT)\n",
    "logger.addHandler(fileHandler)\n",
    "\n",
    "consoleHandler = logging.StreamHandler()\n",
    "consoleHandler.setFormatter(FORMAT)\n",
    "logger.addHandler(consoleHandler)\n",
    "\n",
    "logger.info(\"Training loop starting...\")\n",
    "\n",
    "# setup training configuration\n",
    "training_steps = 32\n",
    "K_epochs = 100\n",
    "ep_len = 10000\n",
    "\n",
    "# create abstraction    \n",
    "action_std = 0.5                    # starting std for action distribution (Multivariate Normal)\n",
    "action_std_decay_rate = 0.05        # linearly decay action_std (action_std = action_std - action_std_decay_rate)\n",
    "min_action_std = 0.1                # minimum action_std (stop decay after action_std <= min_action_std)\n",
    "action_std_decay_freq = 4           # action_std decay frequency (in num training steps)\n",
    "action_std_compute = utils.linear_decay(action_std, action_std_decay_rate, min_action_std)\n",
    "\n",
    "# environment configuration\n",
    "starting_balance = 1000000.0        # starting portfolio amount in dollars\n",
    "max_trade = 10000.0                 # max number of $ amount for buy/sell\n",
    "trading_fee = 0.01                  # trading fee during buy\n",
    "history = 4                         # number of stacks in state\n",
    "reward_scaling = 10 ** -3           # scale the reward signal down\n",
    "\n",
    "# data loading\n",
    "data = preprocessing.load_data()\n",
    "\n",
    "# generate environments\n",
    "num_envs = 4\n",
    "envs = []\n",
    "for i in range(num_envs): \n",
    "    envs.append(environments.CryptoEnv(data, starting_balance, max_trade, trading_fee, history))\n",
    "state = envs[0].get_state(flatten=True)\n",
    "\n",
    "# generate validation environment\n",
    "venv = environments.CryptoEnv(data, starting_balance, max_trade, trading_fee, history)\n",
    "validate_freq = 5\n",
    "\n",
    "state_dim = state.shape[0]\n",
    "action_dim = 5\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "if(torch.cuda.is_available()): \n",
    "    device = torch.device('cuda:0') \n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Device set to : \" + str(torch.cuda.get_device_name(device)))\n",
    "else:\n",
    "    print(\"Device set to : cpu\")\n",
    "\n",
    "pretrained = False\n",
    "pretrained_path = None\n",
    "model_save_path = os.path.join(logDir, \"checkpoints/\")\n",
    "os.mkdir(model_save_path)\n",
    "model_save_freq = 4\n",
    "\n",
    "plot_save_path = os.path.join(logDir, \"plots/\")\n",
    "os.mkdir(plot_save_path)\n",
    "plot_save_freq = 1\n",
    "\n",
    "for i in range(num_envs):\n",
    "    os.mkdir(os.path.join(plot_save_path, 'env' + str(i).zfill(2)))\n",
    "\n",
    "logger.info(f'Training steps: {training_steps}')\n",
    "logger.info(f'Model Optimization epochs: {K_epochs}')\n",
    "logger.info(f'Episode length: {ep_len}')\n",
    "logger.info(f'Action std init: {action_std}')\n",
    "logger.info(f'Action std decay: {action_std_decay_rate}')\n",
    "logger.info(f'Min action std: {min_action_std}')\n",
    "logger.info(f'Action std decay freq: {action_std_decay_freq}')\n",
    "\n",
    "logger.info(f'Starting balance: {starting_balance}')\n",
    "logger.info(f'Maximum trade action: {max_trade}')\n",
    "logger.info(f'Trading fee: {trading_fee}')\n",
    "logger.info(f'State History: {history}')\n",
    "logger.info(f'Reward Scaling: {reward_scaling}')\n",
    "\n",
    "logger.info(f'State dimension: {state_dim}')\n",
    "logger.info(f'Action dimension: {action_dim}')\n",
    "\n",
    "logger.info(f'Pytoch device: {device}')\n",
    "logger.info(f'Pretrained: {pretrained}')\n",
    "if pretrained:\n",
    "    logger.info(f'Pretrained model path: {pretrained_path}')\n",
    "logger.info(f'Model save path: {model_save_path}')\n",
    "logger.info(f'Model save frequecy: {model_save_freq}')\n",
    "\n",
    "# setup actor critic networks\n",
    "actor = models.ActorNN(state_dim, action_dim, [512, 256, 256], device)\n",
    "critic = models.CriticNN(state_dim, action_dim, [512, 256], device)\n",
    "lr_actor = 1e-4      # learning rate for actor network\n",
    "lr_critic = 1e-4     # learning rate for critic network\n",
    "\n",
    "logger.info('Actor: ')\n",
    "logger.info(actor)\n",
    "logger.info(f'Actor LR: {lr_actor}')\n",
    "logger.info('Critic: ')\n",
    "logger.info(critic)\n",
    "logger.info(f'Critic LR: {lr_critic}')\n",
    "\n",
    "# setup training agent\n",
    "agent_name = 'PPO'\n",
    "# PPO settings\n",
    "eps_clip = 0.2          # clip parameter for PPO\n",
    "gamma = 0.99            # discount factor\n",
    "\n",
    "logger.info(f'Agent Policy: {agent_name}')\n",
    "logger.info(f'Epsilon clip: {eps_clip}')\n",
    "logger.info(f'Gamma: {gamma}')\n",
    "\n",
    "if agent_name == 'PPO':\n",
    "    agent = agents.PPO(state_dim, action_dim, actor, critic, lr_actor, lr_critic,\n",
    "    num_envs, gamma, K_epochs, eps_clip, action_std, device)\n",
    "else:\n",
    "    agent = None\n",
    "\n",
    "if pretrained:\n",
    "    agent.load(pretrained_path)\n",
    "    logger.info(f'Loaded saved model: {pretrained_path}')\n",
    "\n",
    "writer = SummaryWriter(logDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wn0zEfC46sLD",
    "outputId": "17579b2a-9359-46be-b640-15836471a6ba"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Time Steps: 320000\n",
      "[INFO] Average Reward:     -436831.642\n",
      "[INFO] Mean Loss:   211.7520, A/C/E:     0.1592,    6.4097,    0.0000\n",
      "[INFO] Action Std: 0.095000000\n",
      "[INFO] Time Steps: 640000\n",
      "[INFO] Average Reward:     -368646.484\n",
      "[INFO] Mean Loss:   204.7524, A/C/E:     0.2012,    7.3488,    0.0000\n",
      "[INFO] Action Std: 0.095000000\n",
      "[INFO] Model Validation: -138587.75\n",
      "[INFO] Mean Val action:  [0.04774256, 0.0527654, -0.029198347, 0.027469758, 0.037212502]\n",
      "[INFO] Time Steps: 960000\n",
      "[INFO] Average Reward:      -42541.545\n",
      "[INFO] Mean Loss:   551.1674, A/C/E:     0.0347,    6.7624,    0.0000\n",
      "[INFO] Action Std: 0.090000000\n",
      "[INFO] Time Steps: 1280000\n",
      "[INFO] Average Reward:     -193233.716\n",
      "[INFO] Mean Loss:   193.2311, A/C/E:     0.0468,    6.2768,    0.0000\n",
      "[INFO] Action Std: 0.085000000\n",
      "[INFO] Model Validation: -141756.25\n",
      "[INFO] Mean Val action:  [0.040110897, 0.046191715, -0.033505034, 0.02256198, 0.037222072]\n",
      "[INFO] Time Steps: 1600000\n",
      "[INFO] Average Reward:     -179289.426\n",
      "[INFO] Mean Loss:   201.6035, A/C/E:    -0.0156,    7.5518,    0.0000\n",
      "[INFO] Action Std: 0.080000000\n",
      "[INFO] Time Steps: 1920000\n",
      "[INFO] Average Reward:     -158274.320\n",
      "[INFO] Mean Loss:   201.2522, A/C/E:     0.0199,    4.6140,    0.0000\n",
      "[INFO] Action Std: 0.075000000\n",
      "[INFO] Model Validation: -141602.25\n",
      "[INFO] Mean Val action:  [0.047770604, 0.045974214, -0.033228282, 0.025245197, 0.0386865]\n",
      "[INFO] Time Steps: 2240000\n",
      "[INFO] Average Reward:     -154471.636\n",
      "[INFO] Mean Loss:   219.0309, A/C/E:     0.1473,    5.0883,    0.0000\n",
      "[INFO] Action Std: 0.070000000\n"
     ]
    }
   ],
   "source": [
    "traj_step = 0\n",
    "time_step = 0\n",
    "max_validation_reward = 0\n",
    "\n",
    "while traj_step <= training_steps:\n",
    "\n",
    "  # collect starting states for environments\n",
    "  trajectory_data = np.zeros((len(envs), ep_len+1, 5 + 1 + 5 + 1), dtype=np.float32)\n",
    "  states = []\n",
    "  for i, env in enumerate(envs):\n",
    "      env.reset()\n",
    "      state = env.get_state(flatten=True)\n",
    "      states.append(state)\n",
    "\n",
    "      price_data = env.get_price_state(False, False)\n",
    "      account_data = env.get_account_state(False)\n",
    "      trajectory_data[i, 0, :5] = price_data[:, 1] # only high price\n",
    "      trajectory_data[i, 0, 5:-1] = account_data\n",
    "      trajectory_data[i, 0, -1] = env.portfolio\n",
    "\n",
    "  states = np.array(states)\n",
    "\n",
    "  # collect ep_len trajectories for each env\n",
    "  average_return = 0\n",
    "\n",
    "  for t in range(ep_len):\n",
    "      actions = agent.select_action(states)\n",
    "      states = []\n",
    "      for i, env in enumerate(envs):\n",
    "          action = actions[i]\n",
    "          reward = env.step(action)\n",
    "          average_return += reward\n",
    "          reward = reward * reward_scaling\n",
    "          agent.buffer.rewards[i].append(reward)\n",
    "          states.append(env.get_state(flatten=True))\n",
    "          time_step += 1\n",
    "\n",
    "          price_data = env.get_price_state(False, False)\n",
    "          account_data = env.get_account_state(False)\n",
    "          trajectory_data[i, 1+t, :5] = price_data[:, 1] # only high price\n",
    "          trajectory_data[i, 1+t, 5:-1] = account_data\n",
    "          trajectory_data[i, 1+t, -1] = env.portfolio\n",
    "\n",
    "      states = np.array(states)\n",
    "\n",
    "  # increment step counter\n",
    "  traj_step += 1\n",
    "\n",
    "  average_return = average_return / num_envs\n",
    "  # update agent using data\n",
    "  median_loss, median_breakdown = agent.update()\n",
    "\n",
    "  logger.info(f'Time Steps: {time_step}')\n",
    "  logger.info(f'Average Reward: {average_return:15.3f}')\n",
    "  logger.info(f'Median Loss: {median_loss:10.4f}')\n",
    "  logger.info(f'Action Std: {agent.action_std:10.9f}')\n",
    "\n",
    "  writer.add_scalar(\"Average Return/Train\", average_return, traj_step)\n",
    "  writer.add_scalar(\"Total Loss/Train\", median_loss, traj_step)\n",
    "  writer.add_scalar(\"Action Std/Train\", agent.action_std, traj_step)\n",
    "  writer.add_scalar(\"Actor Loss/Train\", median_breakdown[0], traj_step)\n",
    "  writer.add_scalar(\"Value Loss/Train\", median_breakdown[1], traj_step)\n",
    "  writer.add_scalar(\"Entropy Loss/Train\", median_breakdown[2], traj_step)\n",
    "\n",
    "  if traj_step % plot_save_freq == 0:\n",
    "      for i in range(num_envs):\n",
    "          utils.plot_trajectory(trajectory_data[i], os.path.join(plot_save_path, 'env' + str(i).zfill(2)), traj_step)\n",
    "\n",
    "  # update agent std\n",
    "  if traj_step % action_std_decay_freq == 0:\n",
    "      index = traj_step // action_std_decay_freq\n",
    "      if index > len(action_std_compute):\n",
    "          index = -1\n",
    "      agent.action_std = action_std_compute[index]\n",
    "      agent.set_action_std(agent.action_std)\n",
    "      agent.scheduler_step()\n",
    "\n",
    "  if traj_step % model_save_freq == 0:\n",
    "      if model_save_path != None:\n",
    "          agent.save(checkpoint_path=os.path.join(model_save_path, \"model\" + str(time_step).zfill(10) + \".pth\"))\n",
    "\n",
    "  if traj_step % validate_freq == 0:\n",
    "      validation_return = 0\n",
    "      for i in range(3):\n",
    "          venv.validate(i)\n",
    "          state = venv.get_state(flatten=True)\n",
    "          for t in range(ep_len):\n",
    "              state = torch.FloatTensor(state).to(device)\n",
    "              with torch.no_grad():\n",
    "                  action = agent.policy.validate(state)\n",
    "                  reward = venv.step(action)\n",
    "                  validation_return += reward\n",
    "                  state = venv.get_state(flatten=True)\n",
    "          \n",
    "      logger.info(f'Validation Reward: {validation_return / 3:15.3f}')\n",
    "      writer.add_scalar(\"Validation Reward/Test\", validation_return / 3, traj_step)\n",
    "      if validation_return > max_validation_reward:\n",
    "          max_validation_reward = validation_return\n",
    "          if model_save_path != None:\n",
    "              agent.save(checkpoint_path=os.path.join(model_save_path, \"model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOKrctOw/5KIA5Wu6Q9noFs",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Crypto.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
