{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Crypto.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOKrctOw/5KIA5Wu6Q9noFs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanchitvohra/crypto-bot/blob/main/Crypto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXLrq_Yy53oF"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sj7WyYBp6OWu",
        "outputId": "ab8b4b6f-8fc4-479c-8243-ddcd27cbb3fe"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eognzY9c6R-C",
        "outputId": "4e2fdac6-0c36-4ee9-f449-2219aae3ca4b"
      },
      "source": [
        "!git clone https://github.com/sanchitvohra/crypto-bot.git\n",
        "!git pull\n",
        "!mkdir /content/crypto-bot/data\n",
        "!cp /content/drive/MyDrive/crypto-bot/crypto_data.npy /content/crypto-bot/data/\n",
        "!mkdir /content/crypto-bot/checkpoints\n",
        "%cd /content/crypto-bot/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'crypto-bot'...\n",
            "remote: Enumerating objects: 140, done.\u001b[K\n",
            "remote: Counting objects: 100% (140/140), done.\u001b[K\n",
            "remote: Compressing objects: 100% (98/98), done.\u001b[K\n",
            "remote: Total 140 (delta 53), reused 115 (delta 30), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (140/140), 28.56 KiB | 835.00 KiB/s, done.\n",
            "Resolving deltas: 100% (53/53), done.\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "/content/crypto-bot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBOmSGqY7D3r"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import logging\n",
        "import os\n",
        "import math\n",
        "\n",
        "import preprocessing\n",
        "import environments\n",
        "import models\n",
        "import agents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dneg05tC8kR_",
        "outputId": "8d79ccb8-5214-444f-ff98-f26a63bd0b05"
      },
      "source": [
        "# Setup logging settings\n",
        "FORMAT = '[%(levelname)s] %(message)s'\n",
        "logging.basicConfig(format=FORMAT)\n",
        "\n",
        "logger = logging.getLogger('common')\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "logger.info(\"Training loop starting...\")\n",
        "\n",
        "\n",
        "# setup training configuration\n",
        "training_steps = 32\n",
        "K_epochs = 100\n",
        "ep_len = 10000\n",
        "action_std = 0.1                    # starting std for action distribution (Multivariate Normal)\n",
        "action_std_decay_rate = 0.005       # linearly decay action_std (action_std = action_std - action_std_decay_rate)\n",
        "min_action_std = 0.005              # minimum action_std (stop decay after action_std <= min_action_std)\n",
        "action_std_decay_freq = 1           # action_std decay frequency (in num training steps)\n",
        "\n",
        "\n",
        "# environment configuration\n",
        "starting_balance = 1000000.0 # starting portfolio amount in dollars\n",
        "max_trade = 100000.0         # max number of $ amount for buy/sell\n",
        "trading_fee = 0.01           # trading fee during buy\n",
        "history = 4                  # number of stacks in state\n",
        "reward_scaling = 10 ** -4    # scale the reward signal down\n",
        "\n",
        "# data loading\n",
        "data = preprocessing.load_data()\n",
        "\n",
        "# generate environments\n",
        "envs = []\n",
        "num_envs = 32\n",
        "for i in range(num_envs): \n",
        "    envs.append(environments.CryptoEnv(data, starting_balance, max_trade, trading_fee, history))\n",
        "state = envs[0].get_state(flatten=True)\n",
        "\n",
        "# generate validation environment\n",
        "venv = environments.CryptoEnv(data, starting_balance, max_trade, trading_fee, history)\n",
        "validate_freq = 2\n",
        "\n",
        "state_dim = state.shape[0]\n",
        "action_dim = 5\n",
        "\n",
        "device = torch.device('cpu')\n",
        "\n",
        "if(torch.cuda.is_available()): \n",
        "    device = torch.device('cuda:0') \n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"Device set to : \" + str(torch.cuda.get_device_name(device)))\n",
        "else:\n",
        "    print(\"Device set to : cpu\")\n",
        "\n",
        "pretrained = False\n",
        "pretrained_path = None\n",
        "model_save_path = \"checkpoints/\"\n",
        "model_save_freq = 4\n",
        "\n",
        "logger.info(f'Training steps: {training_steps}')\n",
        "logger.info(f'Model Optimization epochs: {K_epochs}')\n",
        "logger.info(f'Episode length: {ep_len}')\n",
        "logger.info(f'Action std init: {action_std}')\n",
        "logger.info(f'Action std decay: {action_std_decay_rate}')\n",
        "logger.info(f'Min action std: {min_action_std}')\n",
        "logger.info(f'Action std decay freq: {action_std_decay_freq}')\n",
        "\n",
        "logger.info(f'Starting balance: {starting_balance}')\n",
        "logger.info(f'Maximum trade action: {max_trade}')\n",
        "logger.info(f'Trading fee: {trading_fee}')\n",
        "logger.info(f'State History: {history}')\n",
        "logger.info(f'Reward Scaling: {reward_scaling}')\n",
        "\n",
        "logger.info(f'State dimension: {state_dim}')\n",
        "logger.info(f'Action dimension: {action_dim}')\n",
        "\n",
        "logger.info(f'Pytoch device: {device}')\n",
        "logger.info(f'Pretrained: {pretrained}')\n",
        "if pretrained:\n",
        "    logger.info(f'Pretrained model path: {pretrained_path}')\n",
        "logger.info(f'Model save path: {model_save_path}')\n",
        "logger.info(f'Model save frequecy: {model_save_freq}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO] Training loop starting...\n",
            "[INFO] Training steps: 32\n",
            "[INFO] Model Optimization epochs: 100\n",
            "[INFO] Episode length: 10000\n",
            "[INFO] Action std init: 0.1\n",
            "[INFO] Action std decay: 0.005\n",
            "[INFO] Min action std: 0.005\n",
            "[INFO] Action std decay freq: 1\n",
            "[INFO] Starting balance: 1000000.0\n",
            "[INFO] Maximum trade action: 100000.0\n",
            "[INFO] Trading fee: 0.01\n",
            "[INFO] State History: 4\n",
            "[INFO] Reward Scaling: 0.0001\n",
            "[INFO] State dimension: 231\n",
            "[INFO] Action dimension: 5\n",
            "[INFO] Pytoch device: cuda:0\n",
            "[INFO] Pretrained: False\n",
            "[INFO] Model save path: checkpoints/\n",
            "[INFO] Model save frequecy: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device set to : Tesla K80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiPOwVi-8x_U",
        "outputId": "e0710863-8a40-488c-e360-dcfde19d897d"
      },
      "source": [
        "# setup actor critic networks\n",
        "actor = models.ActorNN(state_dim, action_dim, [1024, 512, 256, 256, 256], device)\n",
        "critic = models.CriticNN(state_dim, action_dim, [1024, 512, 256, 256, 256], device)\n",
        "lr_actor = 1e-6      # learning rate for actor network\n",
        "lr_critic = 1e-6     # learning rate for critic network\n",
        "\n",
        "logger.info('Actor: ')\n",
        "logger.info(actor)\n",
        "logger.info(f'Actor LR: {lr_actor}')\n",
        "logger.info('Critic: ')\n",
        "logger.info(critic)\n",
        "logger.info(f'Critic LR: {lr_critic}')\n",
        "\n",
        "# setup training agent\n",
        "agent_name = 'PPO'\n",
        "# PPO settings\n",
        "eps_clip = 0.2          # clip parameter for PPO\n",
        "gamma = 0.99            # discount factor\n",
        "\n",
        "logger.info(f'Agent Policy: {agent_name}')\n",
        "logger.info(f'Epsilon clip: {eps_clip}')\n",
        "logger.info(f'Gamma: {gamma}')\n",
        "\n",
        "if agent_name == 'PPO':\n",
        "    agent = agents.PPO(state_dim, action_dim, actor, critic, lr_actor, lr_critic,\n",
        "    num_envs, gamma, K_epochs, eps_clip, action_std, device)\n",
        "else:\n",
        "    agent = None\n",
        "\n",
        "if pretrained:\n",
        "  agent.load(pretrained_path)\n",
        "  logger.info(f'Loaded saved model: {pretrained_path}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO] Actor: \n",
            "[INFO] ActorNN(\n",
            "  (model): Sequential(\n",
            "    (0): Linear(in_features=231, out_features=1024, bias=True)\n",
            "    (1): Tanh()\n",
            "    (2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "    (3): Tanh()\n",
            "    (4): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (5): Tanh()\n",
            "    (6): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (7): Tanh()\n",
            "    (8): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (9): Tanh()\n",
            "    (10): Linear(in_features=256, out_features=5, bias=True)\n",
            "    (11): Tanh()\n",
            "  )\n",
            ")\n",
            "[INFO] Actor LR: 1e-06\n",
            "[INFO] Critic: \n",
            "[INFO] CriticNN(\n",
            "  (model): Sequential(\n",
            "    (0): Linear(in_features=231, out_features=1024, bias=True)\n",
            "    (1): Tanh()\n",
            "    (2): Linear(in_features=1024, out_features=512, bias=True)\n",
            "    (3): Tanh()\n",
            "    (4): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (5): Tanh()\n",
            "    (6): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (7): Tanh()\n",
            "    (8): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (9): Tanh()\n",
            "    (10): Linear(in_features=256, out_features=1, bias=True)\n",
            "  )\n",
            ")\n",
            "[INFO] Critic LR: 1e-06\n",
            "[INFO] Agent Policy: PPO\n",
            "[INFO] Epsilon clip: 0.2\n",
            "[INFO] Gamma: 0.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wn0zEfC46sLD",
        "outputId": "17579b2a-9359-46be-b640-15836471a6ba"
      },
      "source": [
        "traj_step = 0\n",
        "time_step = 0\n",
        "max_validation_reward = 0\n",
        "\n",
        "while traj_step <= training_steps:\n",
        "\n",
        "    # collect starting states for environments\n",
        "    states = []\n",
        "    for env in envs:\n",
        "        env.reset()\n",
        "        state = env.get_state(flatten=True)\n",
        "        states.append(state)\n",
        "\n",
        "    states = np.array(states)\n",
        "\n",
        "    # collect ep_len trajectories for each env\n",
        "    average_return = 0\n",
        "    for t in range(ep_len):\n",
        "        actions = agent.select_action(states)\n",
        "        states = []\n",
        "        for i, env in enumerate(envs):\n",
        "            action = actions[i]\n",
        "            reward = env.step(action)\n",
        "            average_return += reward\n",
        "            reward = reward * reward_scaling\n",
        "            agent.buffer.rewards[i].append(reward)\n",
        "            states.append(env.get_state(flatten=True))\n",
        "            time_step += 1\n",
        "\n",
        "        states = np.array(states)\n",
        "    \n",
        "    # increment step counter\n",
        "    traj_step += 1\n",
        "    average_return = average_return / num_envs\n",
        "\n",
        "    # update agent using data\n",
        "    mean_loss = agent.update()\n",
        "\n",
        "    agent_std = agent.action_std\n",
        "\n",
        "    logger.info(f'Time Steps: {time_step}')\n",
        "    logger.info(f'Average Reward: {average_return:15.3f}')\n",
        "    logger.info(f'Mean Loss: {mean_loss[0]:10.4f}, A/C/E: {mean_loss[1]:10.4f},{mean_loss[2]:10.4f},{mean_loss[3]:10.4f}')\n",
        "    logger.info(f'Action Std: {agent_std:10.9f}')\n",
        "\n",
        "    # update agent std\n",
        "    if traj_step % action_std_decay_freq == 0:\n",
        "      agent.action_std = action_std - traj_step * action_std_decay_rate\n",
        "      if agent.action_std < min_action_std:\n",
        "        agent.action_std = min_action_std\n",
        "      agent.set_action_std(agent.action_std)\n",
        "\n",
        "    if traj_step % model_save_freq == 0:\n",
        "      if model_save_path != None:\n",
        "        agent.save(checkpoint_path=os.path.join(model_save_path, \"model\" + str(time_step).zfill(10) + \".pth\"))\n",
        "\n",
        "    if traj_step % validate_freq == 0:\n",
        "      venv.validate()\n",
        "      state = venv.get_state(flatten=True)\n",
        "      validation_return = 0\n",
        "\n",
        "      mean_val_action = np.zeros(action_dim, dtype=np.float32)\n",
        "      for t in range(ep_len):\n",
        "          state = torch.FloatTensor(state).to(device)\n",
        "          with torch.no_grad():\n",
        "              action = agent.policy.validate(state)\n",
        "              mean_val_action += action\n",
        "              reward = venv.step(action)\n",
        "              validation_return += reward\n",
        "              state = venv.get_state(flatten=True)\n",
        "    \n",
        "      mean_val_action /= ep_len\n",
        "      mean_val_action = str(list(mean_val_action))\n",
        "      logger.info(f'Model Validation: {validation_return}')\n",
        "      logger.info(f'Mean Val action:  {mean_val_action}')\n",
        "      if validation_return > max_validation_reward:\n",
        "          max_validation_reward = validation_return\n",
        "          if model_save_path != None:\n",
        "              agent.save(checkpoint_path=os.path.join(model_save_path, \"model.pth\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO] Time Steps: 320000\n",
            "[INFO] Average Reward:     -436831.642\n",
            "[INFO] Mean Loss:   211.7520, A/C/E:     0.1592,    6.4097,    0.0000\n",
            "[INFO] Action Std: 0.095000000\n",
            "[INFO] Time Steps: 640000\n",
            "[INFO] Average Reward:     -368646.484\n",
            "[INFO] Mean Loss:   204.7524, A/C/E:     0.2012,    7.3488,    0.0000\n",
            "[INFO] Action Std: 0.095000000\n",
            "[INFO] Model Validation: -138587.75\n",
            "[INFO] Mean Val action:  [0.04774256, 0.0527654, -0.029198347, 0.027469758, 0.037212502]\n",
            "[INFO] Time Steps: 960000\n",
            "[INFO] Average Reward:      -42541.545\n",
            "[INFO] Mean Loss:   551.1674, A/C/E:     0.0347,    6.7624,    0.0000\n",
            "[INFO] Action Std: 0.090000000\n",
            "[INFO] Time Steps: 1280000\n",
            "[INFO] Average Reward:     -193233.716\n",
            "[INFO] Mean Loss:   193.2311, A/C/E:     0.0468,    6.2768,    0.0000\n",
            "[INFO] Action Std: 0.085000000\n",
            "[INFO] Model Validation: -141756.25\n",
            "[INFO] Mean Val action:  [0.040110897, 0.046191715, -0.033505034, 0.02256198, 0.037222072]\n",
            "[INFO] Time Steps: 1600000\n",
            "[INFO] Average Reward:     -179289.426\n",
            "[INFO] Mean Loss:   201.6035, A/C/E:    -0.0156,    7.5518,    0.0000\n",
            "[INFO] Action Std: 0.080000000\n",
            "[INFO] Time Steps: 1920000\n",
            "[INFO] Average Reward:     -158274.320\n",
            "[INFO] Mean Loss:   201.2522, A/C/E:     0.0199,    4.6140,    0.0000\n",
            "[INFO] Action Std: 0.075000000\n",
            "[INFO] Model Validation: -141602.25\n",
            "[INFO] Mean Val action:  [0.047770604, 0.045974214, -0.033228282, 0.025245197, 0.0386865]\n",
            "[INFO] Time Steps: 2240000\n",
            "[INFO] Average Reward:     -154471.636\n",
            "[INFO] Mean Loss:   219.0309, A/C/E:     0.1473,    5.0883,    0.0000\n",
            "[INFO] Action Std: 0.070000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhpfCqDY_Jdy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}